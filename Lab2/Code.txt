import numpy as np
import matplotlib.pyplot as plt

# Prepare XOR dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def tanh(x):
    return np.tanh(x)

def tanh_derivative(x):
    return 1 - np.tanh(x)**2

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# Neural Network Class
class SimpleNN:
    def __init__(self, activation_function='sigmoid'):
        self.input_size = 2
        self.hidden_size = 2
        self.output_size = 1
        
        self.activation_function = activation_function
        
        # Weights initialization
        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size)
        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size)
    
    def forward(self, X):
        self.hidden_layer_activation = np.dot(X, self.weights_input_hidden)
        self.hidden_layer_output = self.activate(self.hidden_layer_activation)
        self.final_output = np.dot(self.hidden_layer_output, self.weights_hidden_output)
        return self.final_output
    
    def activate(self, x):
        if self.activation_function == 'sigmoid':
            return sigmoid(x)
        elif self.activation_function == 'tanh':
            return tanh(x)
        elif self.activation_function == 'relu':
            return relu(x)
    
    def backpropagate(self, X, y, learning_rate=0.1):
        # Calculate the error
        error = y - self.final_output
        d_final_output = error * self.activate_derivative(self.final_output)

        # Backpropagation
        error_hidden_layer = d_final_output.dot(self.weights_hidden_output.T)
        d_hidden_layer = error_hidden_layer * self.activate_derivative(self.hidden_layer_output)

        # Update weights
        self.weights_hidden_output += self.hidden_layer_output.T.dot(d_final_output) * learning_rate
        self.weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate
    
    def activate_derivative(self, x):
        if self.activation_function == 'sigmoid':
            return sigmoid_derivative(x)
        elif self.activation_function == 'tanh':
            return tanh_derivative(x)
        elif self.activation_function == 'relu':
            return relu_derivative(x)

    def train(self, X, y, epochs=1000):
        for _ in range(epochs):
            self.forward(X)
            self.backpropagate(X, y)

# Train and evaluate the neural network with different activation functions
activation_functions = ['sigmoid', 'tanh', 'relu']
results = {}

for activation in activation_functions:
    nn = SimpleNN(activation_function=activation)
    nn.train(X, y)
    
    # Predictions
    predictions = nn.forward(X)
    predictions = np.round(predictions).astype(int)
    
    # Accuracy
    accuracy = np.mean(predictions == y) * 100
    results[activation] = accuracy
    
    # Print predictions
    print(f"\nPredictions with {activation.capitalize()}:")
    for i in range(len(X)):
        print(f"Input: {X[i]} => Predicted Output: {predictions[i][0]}, Actual Output: {y[i][0]}")

# Print accuracy results
print("\nAccuracy with Different Activation Functions:")
for activation, accuracy in results.items():
    print(f"{activation.capitalize()}: {accuracy:.2f}%")

# Visualize activation functions
def plot_activation_functions():
    x = np.linspace(-10, 10, 400)

    plt.figure(figsize=(10, 6))

    plt.subplot(2, 2, 1)
    plt.plot(x, sigmoid(x), label='Sigmoid')
    plt.title('Sigmoid Function')
    plt.grid()

    plt.subplot(2, 2, 2)
    plt.plot(x, tanh(x), label='Tanh', color='orange')
    plt.title('Tanh Function')
    plt.grid()

    plt.subplot(2, 2, 3)
    plt.plot(x, relu(x), label='ReLU', color='green')
    plt.title('ReLU Function')
    plt.grid()

    plt.tight_layout()
    plt.show()

plot_activation_functions()
