{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfd8s3wCKfTo",
        "outputId": "594a07e8-a8c9-46c1-b456-a9a891b90777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 6.1383771896362305\n",
            "Batch 100: Loss = 3.5806150436401367\n",
            "Batch 200: Loss = 3.0211429595947266\n",
            "Batch 300: Loss = 2.921473503112793\n",
            "Batch 400: Loss = 2.755136728286743\n",
            "Batch 500: Loss = 2.5830039978027344\n",
            "Batch 600: Loss = 2.663576602935791\n",
            "Batch 700: Loss = 2.1128323078155518\n",
            "Batch 800: Loss = 2.9511678218841553\n",
            "Batch 900: Loss = 2.8296451568603516\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Linear(28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "# Generate adversarial examples\n",
        "def generate_adversarial(model, data, target, epsilon):\n",
        "    data.requires_grad = True\n",
        "    output = model(data)\n",
        "    loss = nn.CrossEntropyLoss()(output, target)\n",
        "    loss.backward()\n",
        "    adversarial_data = data + epsilon * data.grad.sign()\n",
        "    return torch.clamp(adversarial_data, 0, 1)\n",
        "\n",
        "\n",
        "# Tangent propagation loss\n",
        "def tangent_prop_loss(model, data, tangent_vectors, target, weight):\n",
        "    output = model(data)\n",
        "    loss = nn.CrossEntropyLoss()(output, target)\n",
        "    tangent_loss = 0.0\n",
        "\n",
        "    for t in tangent_vectors:\n",
        "        # Expand tangent vector to match the batch size\n",
        "        tangent_noise = t.expand_as(data)\n",
        "        tangent_loss += ((model(data + tangent_noise) - output) ** 2).mean()\n",
        "\n",
        "    return loss + weight * tangent_loss\n",
        "\n",
        "\n",
        "# Tangent distance calculation\n",
        "def tangent_distance(x1, x2, tangents):\n",
        "    return min(np.linalg.norm((x1 - x2 - t.numpy())) for t in tangents)\n",
        "\n",
        "\n",
        "# Tangent classifier\n",
        "def tangent_classifier(x, training_data, labels, tangents):\n",
        "    distances = [tangent_distance(x, train_x, tangents) for train_x in training_data]\n",
        "    return labels[np.argmin(distances)]\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def train(model, train_loader, optimizer, epsilon, tangents, weight):\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Generate adversarial examples\n",
        "        adv_data = generate_adversarial(model, data, target, epsilon)\n",
        "\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute losses\n",
        "        loss = tangent_prop_loss(model, data, tangents, target, weight) + \\\n",
        "               nn.CrossEntropyLoss()(model(adv_data), target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item()}\")\n",
        "\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Data transformation\n",
        "    transform = transforms.ToTensor()\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model and optimizer\n",
        "    model = SimpleNN().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Generate tangent vectors matching the shape of input data\n",
        "    tangents = [torch.randn(1, 1, 28, 28, device=device) * 0.1 for _ in range(5)]\n",
        "\n",
        "    # Train the model\n",
        "    train(model, train_loader, optimizer, epsilon=0.2, tangents=tangents, weight=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xiY6Z4DgK7A_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}