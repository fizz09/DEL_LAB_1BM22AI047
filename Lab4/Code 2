#how does a feed forward neural network handle multi class multification tasks and what are the key steps involved in propogating input network to the network to produce classs probabilities using activation function like softmax

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target.reshape(-1, 1)
# One-hot encode the target variable
encoder = OneHotEncoder()
y_onehot = encoder.fit_transform(y).toarray()  # Convert to array
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)
# Initialize parameters
input_size = X.shape[1]
hidden_size = 5
output_size = y_onehot.shape[1]
# Initialize weights and biases
np.random.seed(42)
weights_input_hidden = np.random.randn(input_size, hidden_size)
bias_hidden = np.random.randn(hidden_size)
weights_hidden_output = np.random.randn(hidden_size, output_size)
bias_output = np.random.randn(output_size)
# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability
    return exp_x / exp_x.sum(axis=1, keepdims=True)
# Training parameters
learning_rate = 0.01
epochs = 10000
# Training loop
for epoch in range(epochs):
    # Forward pass
    hidden_layer_input = np.dot(X_train, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    predicted_output = softmax(output_layer_input)
    # Compute loss (cross-entropy)
    loss = -np.mean(np.sum(y_train * np.log(predicted_output + 1e-9), axis=1))  # Small constant for stability
    # Backpropagation
    output_error = predicted_output - y_train
    hidden_error = np.dot(output_error, weights_hidden_output.T) * hidden_layer_output * (1 - hidden_layer_output)
    # Update weights and biases
    weights_hidden_output -= learning_rate * np.dot(hidden_layer_output.T, output_error) / X_train.shape[0]
    bias_output -= learning_rate * np.mean(output_error, axis=0)
    weights_input_hidden -= learning_rate * np.dot(X_train.T, hidden_error) / X_train.shape[0]
    bias_hidden -= learning_rate * np.mean(hidden_error, axis=0)
    # Print loss every 1000 epochs
    if epoch % 1000 == 0:
        print(f'Epoch {epoch}, Loss: {loss:.4f}')
# Predictions on test set
hidden_layer_input = np.dot(X_test, weights_input_hidden) + bias_hidden
hidden_layer_output = sigmoid(hidden_layer_input)
output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
predicted_output = softmax(output_layer_input)
# Get predicted class labels
predicted_classes = np.argmax(predicted_output, axis=1)
true_classes = np.argmax(y_test, axis=1)
# Accuracy
accuracy = np.mean(predicted_classes == true_classes)
print(f'Accuracy: {accuracy * 100:.2f}%')
